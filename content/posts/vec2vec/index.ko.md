---
title: vec2vec, 서로 다른 공간에서 온 임베딩을 번역하다
date: 2025-05-26T20:38:11+09:00
draft: false
author: pizzathief
description:
summary: Harnessing the Universal Geometry of Embeddings (2025) 빠르게 읽어보기
keywords:
  - vec2vec
  - embedding
  - 임베딩
tags:
  - AI-ML
categories:
  - data
slug: vec2vec
ShowReadingTime: true
ShowShareButtons: true
ShowPostNavLinks: false
ShowBreadCrumbs: false
ShowCodeCopyButtons: true
ShowWordCount: false
ShowRssButtonInSectionTermList: true
disableScrollToTop: false
hidemeta: false
hideSummary: false
showtoc: true
tocopen: false
UseHugoToc: true
disableShare: false
searchHidden: false
robotsNoIndex: false
comments: true
weight: 10
math: true
---

딥러닝의 세계에서 임베딩 벡터가 핵심적인 존재가 된 건 꽤 오래 된 일입니다. 기본적으로 모델은 숫자를 요구하기 때문에, 원본 데이터가 이미지든 텍스트든 간에 그 의미를 잘 함축하고 있는(representation) 작은 크기의 dense한 수치형 벡터로 변환하는 것은 딥러닝식 문제 풀이의 가장 첫번째 단계라고 할 수 있죠. 아주 고전적인 모델부터 최신의 LLM까지 이런 임베딩을 통해 유사성을 계산하고 의미 기반의 연산을 진행할 수 있게 되는 것입니다.

{{< figure src="/posts/vec2vec/embedding-vector.png" width="500" caption="[출처](https://www.pinecone.io/learn/vector-embeddings/)" align="center">}}


임베딩을 이야기할 때 종종 등장하는 표현이 있습니다.

> 임베딩이 같은 공간에 있지 않다, align이 되지 않았다

어떤 의미냐면, 서로 다른 모델로 학습된 임베딩은 서로 다른 임베딩 공간에 분포하게 됩니다. 텍스트로 예시를 들면 완전히 같은 단어(dog)라도 A모델로 학습된 dog 벡터와 B모델로 학습된 dog 벡터는 다른 값을 가질 것입니다. 그렇기 때문에 A모델로 학습된 dog 벡터와 B모델로 학습된 cat벡터를 코사인 유사도 같은 메트릭으로 거리를 잰 다음 가깝지 않다고 뭐라고 할 수는 없습니다. 마치 완전히 다른 외국어를 사용하는 것처럼 두 임베딩은 같은 의미 체계를 공유하지 않기 때문이죠. 따라서 **다른 임베딩 공간에서 얻은 임베딩 벡터는 별도의 정렬 작업 없이 직접 비교하거나 짬뽕해서 사용하면 안 된다** 라는 게 일반적으로 알려져 있는 주의사항입니다.

<br>

## 플라톤(갑자기?) 표현 이론

[The Platonic Representation Hypothesis](https://arxiv.org/abs/2405.07987) 라는 논문은 이 주제에 대해 상당히 재미있는 이론을 제시합니다. 그건 바로 충분한 크기의 데이터로 학습된 representation 모델들은 서로 다른 아키텍쳐로 따로 학습되었을지라도 결국 동일한 표현을 학습하게 된다는 것입니다.


{{< figure src="/posts/vec2vec/platonic-representation-hypothesis.png" width="400" caption="본질과 그림자" align="center">}}



뜬금 없이 플라톤이 왜 나오냐 하면, 아마 학교 다닐 때 한번쯤은 들어봤을 이데아 이론 때문인데요. 이데아라고 부를 수 있는 변하지 않는 본질이 존재하고, 우리가 실제로 감각하고 보는 것들은 마치 어두운 동굴 속에 갇힌 사람들이 벽에 비치는 사물의 그림자를 보는 것과 같은, 이데아의 불완전한 묘사라는 이론입니다(만 알아도 됩니다. 저도 잘 모릅니다). 위 그림에서 보면 Z라는 현실(본질적인 특성)이 존재할 때, 이미지와 텍스트는 이를 각각 다른 방식으로 묘사하는 것을 알 수 있습니다. 이미지와 텍스트를 통해 학습한 모델은 이 현실 Z의 projection을 관찰하여 정보를 얻게 되는 거죠. 

어쨌든 그 뒤에는 하나의 진실(본질; 통계적 속성)이 존재하기 때문에, 데이터와 모델의 크기가 커질수록, 그리고 downstream task에서 잘 작동하는 유능한 모델이 될수록, 여러 임베딩이 하나의 표현으로 수렴하게 된다는 것이 이 논문이 제시하는 가설입니다. 실제로 굉장히 다양한 방식으로 훈련된 임베딩에 대해 alignment 지표를 관찰한 결과를 간접적인 증거로 같이 제시하고 있습니다.

그냥 본질이 똑같애서 그래 말고 구체적으로 왜 그렇게 되는가?에 대한 답변은 다음과 같습니다.
- Task Generality : 데이터가 대규모이고 모델의 태스크가 범용적일(어려울) 수록 제약이 많아지고 결국 그 제약을 만족시킬 수 있는 가능한 표현(solution)의 공간은 더더욱 작아짐(즉 도달 가능한 목표 공간은 거기서 거기일 것임)
- Model Capacity : 모델이 커질수록 최적해를 더 효과적으로 찾을 수 있기 때문에 충분히 유능한 모델은 수렴할 가능성이 높음.
- Simplicity Bias : 입출력이 동일하더라도 내부 표현은 다르게 학습할 수 있지만 딥한 네트워크는 본질적으로 가장 simple fit을 찾도록 편향되어 있기 때문에 여러 가능한 복잡한 표현 중에서도 공유된 solution 공간으로 수렴할 가능성이 큼.

<br>

## vec2vec

[Harnessing the Universal Geometry of Embeddings](https://arxiv.org/html/2505.12540v2) 는 이 플라톤 표현 이론을 조금 더 발전시켜서 실제로 완전히 서로 다른 모델로 학습된 두 개의 임베딩을 (인코더를 모르더라도, 쌍으로 맵핑된 정답 없이도 **완전히 비지도 방식으로**) 성공적으로 연결할 수 있다는 것을 보여줍니다. 범용적으로 사용할 수 있는 임베딩 번역기 솔루션인 거죠. 이걸 **vec2vec**이라고 부릅니다.


{{< figure src="/posts/vec2vec/vec2vec-architecture.png" width="580" caption="캡션" align="center">}}


### 구조

vec2vec의 구조는 위 그림과 같고, 입력 어댑터(A), 출력 어댑터(B), 공유된 backbone 네트워크(T) 이렇게 3가지 어댑터로 쪼개지는 구조입니다.  우리는 서로 다른 모델에서 학습된 2개의 임베딩을 연결하는 게 목표이니까 각각 임베딩에 사용된 모델1, 모델2가 있고 한쪽의 인코더는 모르는 상황이라고 합시다. 기본적인 가정은, 각 임베딩들을 어떤 보편적인 잠재 표현(universal latent representation; 플라톤 표현 이론에서 생각하는 "본질" 적인 표현)으로 맵핑할 수 있다는 것입니다.

- 입력 어댑터 A: 각 개별 임베딩 공간에서 나온 임베딩을 보편적인 잠재 표현으로 변환하는 역할
- Shared Backbone Network T: 중간에서 잠재 공간 내에서 임베딩을 변환하는 역할
- 입력 어댑터 B: 보편적인 잠재 표현에서 나온 임베딩을 각 개별 임베딩 공간의 표현으로 변환하는 역할

residual connections, layer normalization, and SiLU nonlinearities 조합의 MLP를 사용했다고 하고요. 위 3개 층을 적절히 조합하면 다음과 같은 4종류의 함수가 나올 수 있습니다. (∘를 일반적인 합성함수 기호로 생각하여 거꾸로 읽으면 됨)

- 변환 함수:
	- F1 = B2 ∘ T ∘ A1: 모델1 공간에서 모델2 공간으로 변환
	- F2 = B1 ∘ T ∘ A2: 모델1 공간에서 모델2 공간으로 변환
- 재구성 함수:
	- R1 = B1 ∘ T ∘ A1: 모델1 공간에서 잠재 공간을 거쳐 다시 모델1 공간으로 재구성
	- R2 = B2 ∘ T ∘ A2: 모델2 공간에서 잠재 공간을 거쳐 다시 모델2 공간으로 재구성

A랑 B는 알겠는데 중간에 T는 왜 필요한가? 라는 생각이 들 수 있는데요, A는 단순히 각 모델에서 잠재 공간으로 일치시켜주는 역할이라면 T는 모델 1,2,3,..등 여러 다른 소스, 다른 도메인에서 온 임베딩들이 잠재 공간 내에서 정말 공통의 표현으로 잘 정렬될 수 있도록 하기 위한 목적의 MLP가 하나 더 있는 거라고 이해하면 됩니다.


### 학습

vec2vec의 목표는 동일한 텍스트에 대해 여러 방식으로 학습된 임베딩이 공유된 보편적인 잠재 표현 기준으로 거의 동일해지는 것입니다. 그렇게 하려면 다음과 같은 여러 가지 세부 목표(손실 함수)를 만족시켜야 합니다.

- 재구성 손실 (Reconstruction Loss) : 임베딩이 잠재 공간을 거쳐 원래 공간으로 돌아왔을 때 온전히 잘 돌아와야 함
- 사이클 일관성 손실 (Cycle Consistency Loss) : 임베딩이 잠재 공간을 거쳐 다른 공간에 갔다가 다시 원래 공간으로 돌아왔을 때 온전히 잘 돌아와야 함
- 벡터 공간 보존 (Vector Space Preservation Loss) : 변환된 임베딩들 간의 pairwise 거리가 변환 전의 임베딩들 간의 거리와 비슷해야 함 (= 임베딩 공간 내의 상대적인 구조가 변환하면서 왜곡되지 않도록)
- 적대적 손실 (Adversarial Loss) : 일반적인 GAN 학습 방식과 동일함. F와 R이 generator라면 discriminator 네트워크를 따로 둬서 변환된(생성된) 임베딩 분포를 타겟 분포와 잘 맞출 수 있도록 함

앞의 두 가지 손실은 2개의 임베딩 공간이 잠재 공간을 사이에 두고 벡터가 이리 갔다 저리 갔다 돌아왔다 난리를 쳐도 안정적으로 값이 유지될 때 우리의 A-T-B 어댑터가 완전하다고 볼 수 있어 라는 논리라는 것을 알 수 있습니다.  제가 번역기에다 대고 한국어로 무슨 문장을 말하고, 그게 번역된 불어 문장을 들은 프랑스인이 (자기가 들은 말을) 똑같이 따라해서 다시 번역기를 거쳐서 저한테 왔을 때, 그게 제가 말한 문장이어야 한단 거죠. GAN loss같은 경우는 ablation(없애보기) 했을 때 성능이 많이 떨어지는 것으로 보아 잠재공간으로 변환하는(새 벡터를 생성하는) 과정에서 핵심적인 역할을 하는 것으로 보인다고 합니다.


### 평가와 결과

{{< figure src="/posts/vec2vec/results.png" width="480"  align="center">}}


다시 vec2vec 번역의 목표를 다른 문장으로 써보면, 소스 임베딩 $u_i = M_1(d_i)$가 주어졌지만 $M_1$이 뭔지는 모르는 상황에서, 동일한 원본 문서에 대한 타겟 모델의 임베딩 $v_i=M_2(d_i)$ 에 최대한 가까운 벡터 $F(u_i)$ 를 생성하는 것입니다. 사용한 평가 지표는 다음과 같습니다.

- Mean Cosine Similarity : 번역된 임베딩 $F(u_i)$가 실제 타겟 임베딩 $v_i$와 얼마나 유사한가 = 1.0이어야 이상적
- Top-1 Accuracy: 번역된 임베딩 $F(u_i)$가 타겟 임베딩 후보 집합 $M_2(d_j)$ 내에서 실제 타겟 임베딩 $v_i$와 가장 가까운(역시 코사인 유사도 기준) 임베딩으로 올바르게 식별되는 비율 = 1.0이어야 이상적
- Mean Rank: 번역된 임베딩 $F(u_i)$가 타겟 임베딩 후보 집합 $M_2(d_j)$ 을 비교했을 때 실제 타겟 임베딩 $v_i$의 평균 순위 = 1위어야 이상적

연구에서 vec2vec은Natural Questions(NQ) 데이터를 통해 학습했는데, 같은 데이터를 통한 평가(in-distrbution) 결과 뿐 아니라 아예 도메인이 다른 트윗 데이터나 의료 기록 데이터에서도 좋은 성능을 보여 일반화 능력이 확인되었습니다.


<br>

## 임베딩도 보안이 필요하다

보통 회사에서 임베딩 데이터를 저장할 때 다른 데이터만큼 보안을 신경써야 한다고 생각하지는 않을 겁니다. 예를 들어 '고객 A는 이름이 OOO이고 28세 여성이고 서울시 도봉구에 살며 상품 A,B,C를 구매했고,...' 이런 정보는 뒷구르기하면서 봐도 유출되면 큰일나는 고객 A의 개인정보입니다. 반면에 어떤 모델을 학습시켜서 유저의 구매 패턴을 나타내는 임베딩을 얻었다고 하면, 하나 뽑아서 관찰했을 때 (0.324, -0.4253, 0.988, ..., 이하 256차원의 벡터) 이런 꼴로 생겼잖아요? 누가 이 데이터를 훔쳐가봤자 무슨 정보인지 도저히 알 도리가 없을 거라는 생각이 들죠.

하지만 vec2vec 논문의 마지막 섹션에서는 vec2vec을 활용해 모르는 모델로 학습된 임베딩의 modality와 언어 정보 정도만 알았을 때에도 그 안에서 얼만큼 정보를 추출해낼 수 있는지를 보여주고 있습니다. 


{{< figure src="/posts/vec2vec/prompt.png" width="480" caption="사용한 프롬프트" align="center">}}

{{< figure src="/posts/vec2vec/inversion.png" width="480"  align="center">}}



이메일 데이터를 이용한 실험인데요, 이전 상황과 동일하게 이메일 텍스트를 알려지지 않은 모델($M_1$)에 넣고 임베딩을 얻은 뒤 vec2vec을 사용하여 알려진 다른 모델($M_2$)의 임베딩으로 변환했습니다. 그다음에 이 변환된 임베딩을 사용하여 원래의 텍스트 이메일 입력을 재구성(zero-shot inversion) 합니다. 그 다음 원본 이메일과 재구성된 이메일을 LLM(GPT-4o)에 넣고 원본 이메일의 정보가 유출되었는지를 yes/no로 물어봅니다.

그 결과 특정 모델 쌍에 대해서는 변환된 임베딩을 통해 거의 문서의 최대 80%까지 정보를 추출했다는 것을 알 수 있고, 재구성된 텍스트가 완벽하지는 않았지만 이름, 날짜, 승진 정보, 재무 정보, 서비스 중단, 심지어 점심 주문과 같은 민감한 정보를 추출할 수 있었다고 합니다. 즉 이 정보 추출 실험을 통해 알 수 있는 것은 vec2vec이 두 개 임베딩 공간을 효율적으로 변환할 수 있고, 그게 **단순히 기하학적인 구조만이 아닌 텍스트의 의미론적인 내용까지 보존한다**는 것입니다. 그리고 임베딩이 '봐도 뭔지 알 수 없는 몇백개의 숫자일 뿐'이라는 생각은 조금 위험할 수도 있겠다는 생각이 들게 하네요.
